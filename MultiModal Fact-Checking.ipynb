{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gJ6-PiDcSk_"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V87R2fyYcKjX",
        "outputId": "fc62b9bc-b1b1-4382-9d0e-7059be09bded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIp6ZSKYziG2"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf0uRufayXQQ",
        "outputId": "ebfc4567-065f-4d29-eab7-78cce4be6750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 30.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 41.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 53.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvOQSseq7NpV",
        "outputId": "bd47b2e8-a404-47f8-9dac-8d81d5688603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.24.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 64.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.10.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=4b2b308f1fbc408df6bf0964f6c74307e04d7bbf44eeece510da6e5470329e58\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzcI9nlDcMqO",
        "outputId": "b7a58a84-17d4-49b7-95d4-aa04567b5248"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.9.24)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTrf_9C9cU6j"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "A1Eg3qArcv6u",
        "outputId": "dafead11-1a0f-451c-fe75-cdcf60466862"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-07f64099-6437-42cc-8174-fda2e805d8e4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-07f64099-6437-42cc-8174-fda2e805d8e4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"vishalnarnaware\",\"key\":\"ec5ca920dc8759f90bc25c5074ea5aee\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzWktsJicnv6"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle                                               \n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMB24HmVdI6R",
        "outputId": "194e58ae-a162-4a1c-ec4a-88d76d4ad793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading factify-data.zip to /content\n",
            "100% 6.46G/6.47G [01:11<00:00, 123MB/s]\n",
            "100% 6.47G/6.47G [01:11<00:00, 97.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d naineshulke/factify-data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akFoInlKIb38"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/factify-data.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/drive/MyDrive/FactifyData\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T7uQgT20dMRK"
      },
      "outputs": [],
      "source": [
        "# !unzip factify-data.zip -d /content/drive/MyDrive/FactifyData"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMQlKL9nnyZr"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orYBcHejnxXd",
        "outputId": "9e1b92d6-9d69-40e7-d014-1f8a313441b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3500, 6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3500/3500 [02:12<00:00, 26.49it/s]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/FactifyData/Clean_Data/train_clean.csv', index_col='Id')[['claim', 'claim_image', 'document', 'document_image', 'Category']]\n",
        "\n",
        "category = {\n",
        "    'Support_Multimodal': 0,\n",
        "    'Support_Text': 1,\n",
        "    'Insufficient_Multimodal': 2,\n",
        "    'Insufficient_Text': 3,\n",
        "    'Refute': 4\n",
        "}\n",
        "\n",
        "df['Label'] = df['Category'].map(category)\n",
        "\n",
        "_, df = train_test_split(df, test_size=0.10, random_state=0, stratify=df[['Label']])\n",
        "\n",
        "print(df.shape)\n",
        "\n",
        "data, ids = {}, []\n",
        "for n, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    path = '/content/drive/MyDrive/FactifyData/Factify_Data/Factify_Images/train/'\n",
        "    filename = path + 'claim/' + row['Category'] + '/' + str(n) + '.jpg'\n",
        "    input_claim_image = Image.open(filename)\n",
        "    claim_image = preprocess(input_claim_image)\n",
        "\n",
        "    filename = path + 'document/' + row['Category'] + '/' + str(n) + '.jpg'\n",
        "    input_document_image = Image.open(filename)\n",
        "    document_image = preprocess(input_document_image)\n",
        "\n",
        "    data[n] = (row['claim'], claim_image, row['document'], document_image, row['Label'])\n",
        "    \n",
        "    \n",
        "with open('/content/drive/MyDrive/FactifyData/processed_train.pickle', 'wb') as file:\n",
        "    pickle.dump(data, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFZMHNeVvIK2"
      },
      "outputs": [],
      "source": [
        "del data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csCXixhrvRqN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/FactifyData/Clean_Data/val_clean.csv', index_col='Id')[['claim', 'claim_image', 'document', 'document_image', 'Category']]\n",
        "\n",
        "category = {\n",
        "    'Support_Multimodal': 0,\n",
        "    'Support_Text': 1,\n",
        "    'Insufficient_Multimodal': 2,\n",
        "    'Insufficient_Text': 3,\n",
        "    'Refute': 4\n",
        "}\n",
        "\n",
        "df['Label'] = df['Category'].map(category)\n",
        "\n",
        "_, df = train_test_split(df, test_size=0.10, random_state=0, stratify=df[['Label']])\n",
        "\n",
        "print(df.shape)\n",
        "\n",
        "data, ids = {}, []\n",
        "for n, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    path = '/content/drive/MyDrive/FactifyData/Factify_Data/Factify_Images/val/'\n",
        "    filename = path + 'claim/' + str(n) + '.jpg'\n",
        "    input_claim_image = Image.open(filename)\n",
        "    claim_image = preprocess(input_claim_image)\n",
        "\n",
        "    filename = path + 'document/' + str(n) + '.jpg'\n",
        "    input_document_image = Image.open(filename)\n",
        "    document_image = preprocess(input_document_image)\n",
        "\n",
        "    data[n] = (row['claim'], claim_image, row['document'], document_image, row['Label'])\n",
        "    \n",
        "    \n",
        "with open('/content/drive/MyDrive/FactifyData/processed_val.pickle', 'wb') as file:\n",
        "    pickle.dump(data, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K62I47Mevgns"
      },
      "outputs": [],
      "source": [
        "del data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subset Dataset"
      ],
      "metadata": {
        "id": "bc7rZXuxXo9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset = 0.01   # 1% for rapid prototyping, 5% for checking, 10% for final training"
      ],
      "metadata": {
        "id": "BKkNyTbFqicn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "content = pd.read_csv('/content/drive/MyDrive/FactifyData/Clean_Data/train_clean.csv')\n",
        "\n",
        "category = {\n",
        "    'Support_Multimodal': 0,\n",
        "    'Support_Text': 1,\n",
        "    'Insufficient_Multimodal': 2,\n",
        "    'Insufficient_Text': 3,\n",
        "    'Refute': 4\n",
        "}\n",
        "\n",
        "content['Label'] = content['Category'].map(category)\n",
        "content = content.dropna(subset=['claim', 'document'])\n",
        "_, content = train_test_split(content, test_size=subset, random_state=0, stratify=content[['Label']])\n",
        "\n",
        "# content.reset_index(inplace=True)\n",
        "\n",
        "# content.head()"
      ],
      "metadata": {
        "id": "hOjf02_DXoY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content.to_csv('/content/drive/MyDrive/FactifyData/Clean_Data/train_clean_'+str(round(subset*100))+'.csv', index=False)"
      ],
      "metadata": {
        "id": "tVf-sZXgY0es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "content = pd.read_csv('/content/drive/MyDrive/FactifyData/Clean_Data/val_clean.csv')\n",
        "\n",
        "category = {\n",
        "    'Support_Multimodal': 0,\n",
        "    'Support_Text': 1,\n",
        "    'Insufficient_Multimodal': 2,\n",
        "    'Insufficient_Text': 3,\n",
        "    'Refute': 4\n",
        "}\n",
        "\n",
        "content['Label'] = content['Category'].map(category)\n",
        "content = content.dropna(subset=['claim', 'document'])\n",
        "_, content = train_test_split(content, test_size=subset, random_state=0, stratify=content[['Label']])\n",
        "\n",
        "# content.reset_index(inplace=True)\n",
        "\n",
        "content.to_csv('/content/drive/MyDrive/FactifyData/Clean_Data/val_clean_'+str(round(subset*100))+'.csv', index=False)\n",
        "\n",
        "content.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "2ErHAOfdZe2x",
        "outputId": "ced11c0d-8eff-40c1-f929-e7e4e175422d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Id                                        claim_image  \\\n",
              "2242  2243  http://pbs.twimg.com/amplify_video_thumb/12145...   \n",
              "6885  6886  http://pbs.twimg.com/media/EgMhP0tUwAIMC4w.png...   \n",
              "2333  2334  http://pbs.twimg.com/amplify_video_thumb/11106...   \n",
              "7019  7020  https://i1.wp.com/smhoaxslayer.com/wp-content/...   \n",
              "3509  3510  http://pbs.twimg.com/media/EgaITrDWsAADaXD.jpg...   \n",
              "\n",
              "                                                  claim  \\\n",
              "2242  sen elizabeth warren escalating tension iran r...   \n",
              "6885  spoken director general national disaster resp...   \n",
              "2333  justice department shifted position agreed low...   \n",
              "7019  video showing physical altercation mla distric...   \n",
              "3509  penny reinvents trump presidency disorienting ...   \n",
              "\n",
              "                                              claim_ocr  \\\n",
              "2242           thepoliticviewthepoliticalobcviewtheview   \n",
              "6885  amit shahthe collapse building raigad maharash...   \n",
              "2333                                            abcnews   \n",
              "7019  bihar bjp mla attack dist collectorshame goon ...   \n",
              "3509                                       unistatesdia   \n",
              "\n",
              "                                         document_image  \\\n",
              "2242  https://cdn.cnn.com/cnnnext/dam/assets/1912311...   \n",
              "6885  https://images.hindustantimes.com/rf/image_siz...   \n",
              "2333  http://pbs.twimg.com/media/D2rUrz5WsAMsscw.jpg...   \n",
              "7019  https://i2.wp.com/smhoaxslayer.com/wp-content/...   \n",
              "3509  http://pbs.twimg.com/media/EgY8nb9WoAYkBAh.jpg...   \n",
              "\n",
              "                                               document  \\\n",
              "2242  opinion eric swalwell updated pm et mon januar...   \n",
              "6885  difficult three month india policy response co...   \n",
              "2333  trump administration monday said entire afford...   \n",
              "7019  video showing physical altercation mla distric...   \n",
              "3509  president trump joined vice president penny fo...   \n",
              "\n",
              "                                           document_ocr  \\\n",
              "2242                                                NaN   \n",
              "6885                                                rry   \n",
              "2333  savedont destroshealthcarerkers centlronsoicus...   \n",
              "7019  nation stako like comment share followk commen...   \n",
              "3509                                       abc newslive   \n",
              "\n",
              "                     Category  Label  \n",
              "2242  Insufficient_Multimodal      2  \n",
              "6885        Insufficient_Text      3  \n",
              "2333        Insufficient_Text      3  \n",
              "7019                   Refute      4  \n",
              "3509        Insufficient_Text      3  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f96d7476-67dd-4e59-bd4e-5e98d40bb26b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>claim_image</th>\n",
              "      <th>claim</th>\n",
              "      <th>claim_ocr</th>\n",
              "      <th>document_image</th>\n",
              "      <th>document</th>\n",
              "      <th>document_ocr</th>\n",
              "      <th>Category</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2242</th>\n",
              "      <td>2243</td>\n",
              "      <td>http://pbs.twimg.com/amplify_video_thumb/12145...</td>\n",
              "      <td>sen elizabeth warren escalating tension iran r...</td>\n",
              "      <td>thepoliticviewthepoliticalobcviewtheview</td>\n",
              "      <td>https://cdn.cnn.com/cnnnext/dam/assets/1912311...</td>\n",
              "      <td>opinion eric swalwell updated pm et mon januar...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Insufficient_Multimodal</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6885</th>\n",
              "      <td>6886</td>\n",
              "      <td>http://pbs.twimg.com/media/EgMhP0tUwAIMC4w.png...</td>\n",
              "      <td>spoken director general national disaster resp...</td>\n",
              "      <td>amit shahthe collapse building raigad maharash...</td>\n",
              "      <td>https://images.hindustantimes.com/rf/image_siz...</td>\n",
              "      <td>difficult three month india policy response co...</td>\n",
              "      <td>rry</td>\n",
              "      <td>Insufficient_Text</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2333</th>\n",
              "      <td>2334</td>\n",
              "      <td>http://pbs.twimg.com/amplify_video_thumb/11106...</td>\n",
              "      <td>justice department shifted position agreed low...</td>\n",
              "      <td>abcnews</td>\n",
              "      <td>http://pbs.twimg.com/media/D2rUrz5WsAMsscw.jpg...</td>\n",
              "      <td>trump administration monday said entire afford...</td>\n",
              "      <td>savedont destroshealthcarerkers centlronsoicus...</td>\n",
              "      <td>Insufficient_Text</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7019</th>\n",
              "      <td>7020</td>\n",
              "      <td>https://i1.wp.com/smhoaxslayer.com/wp-content/...</td>\n",
              "      <td>video showing physical altercation mla distric...</td>\n",
              "      <td>bihar bjp mla attack dist collectorshame goon ...</td>\n",
              "      <td>https://i2.wp.com/smhoaxslayer.com/wp-content/...</td>\n",
              "      <td>video showing physical altercation mla distric...</td>\n",
              "      <td>nation stako like comment share followk commen...</td>\n",
              "      <td>Refute</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3509</th>\n",
              "      <td>3510</td>\n",
              "      <td>http://pbs.twimg.com/media/EgaITrDWsAADaXD.jpg...</td>\n",
              "      <td>penny reinvents trump presidency disorienting ...</td>\n",
              "      <td>unistatesdia</td>\n",
              "      <td>http://pbs.twimg.com/media/EgY8nb9WoAYkBAh.jpg...</td>\n",
              "      <td>president trump joined vice president penny fo...</td>\n",
              "      <td>abc newslive</td>\n",
              "      <td>Insufficient_Text</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f96d7476-67dd-4e59-bd4e-5e98d40bb26b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f96d7476-67dd-4e59-bd4e-5e98d40bb26b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f96d7476-67dd-4e59-bd4e-5e98d40bb26b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt_36wCoRw1Q",
        "outputId": "c9de3a9a-f03a-40a2-c0fa-c15b28472505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g6cevNnRtB9",
        "outputId": "2b0862f8-fdeb-4b3b-e31c-464aee8f53b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Id                 0\n",
              "claim_image        0\n",
              "claim              0\n",
              "claim_ocr         13\n",
              "document_image     0\n",
              "document           0\n",
              "document_ocr      19\n",
              "Category           0\n",
              "Label              0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjygNkjQzD3i"
      },
      "source": [
        "## Define layers and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJuHhOUEv5bh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    ''' Scaled Dot-Product Attention '''\n",
        "\n",
        "    def __init__(self, temperature, attn_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
        "\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
        "        output = torch.matmul(attn, v)\n",
        "\n",
        "        return output, attn\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    ''' Multi-Head Attention module '''\n",
        "\n",
        "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "\n",
        "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
        "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
        "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
        "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
        "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
        "\n",
        "        residual = q\n",
        "\n",
        "        # Pass through the pre-attention projection: b x lq x (n*dv)\n",
        "        # Separate different heads: b x lq x n x dv\n",
        "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
        "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
        "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
        "\n",
        "        # Transpose for attention dot product: b x n x lq x dv\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
        "\n",
        "        q, attn = self.attention(q, k, v, mask=mask)\n",
        "\n",
        "        # Transpose to move the head dimension back: b x lq x n x dv\n",
        "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
        "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
        "        q = self.dropout(self.fc(q))\n",
        "        q += residual\n",
        "\n",
        "        q = self.layer_norm(q)\n",
        "\n",
        "        return q, attn\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    ''' A two-feed-forward-layer module '''\n",
        "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.w_1 = nn.Linear(d_in, d_hid)\n",
        "        self.w_2 = nn.Linear(d_hid, d_in)\n",
        "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        x = self.w_2(F.gelu(self.w_1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x += residual\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvKCIKNnvt5q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "\n",
        "class Mish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.tanh(F.softplus(x))\n",
        "\n",
        "\n",
        "class FakeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        dim = 512\n",
        "        dropout = 0.1\n",
        "        head = 4\n",
        "\n",
        "        self.text_embedding = nn.Sequential(\n",
        "            nn.Linear(768, dim),\n",
        "            Mish()\n",
        "            # nn.ReLU()\n",
        "        )\n",
        "        self.document_text_embedding = nn.Sequential(\n",
        "            nn.Linear(768, dim),\n",
        "            Mish()\n",
        "            # nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.image_embedding = nn.Sequential(\n",
        "            nn.Linear(768, dim),\n",
        "            Mish()\n",
        "            # nn.ReLU()\n",
        "        )\n",
        "        self.document_image_embedding = nn.Sequential(\n",
        "            nn.Linear(768, dim),\n",
        "            Mish()\n",
        "            # nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.claim_document_text_attention = MultiHeadAttention(head, dim, dim, dim, dropout=dropout)\n",
        "        self.claim_document_text_pos_ffn = PositionwiseFeedForward(dim, dim*2, dropout=dropout)\n",
        "\n",
        "        self.claim_document_image_attention = MultiHeadAttention(head, dim, dim, dim, dropout=dropout)\n",
        "        self.claim_document_image_pos_ffn = PositionwiseFeedForward(dim, dim*2, dropout=dropout)\n",
        "\n",
        "        self.text_image_attention = MultiHeadAttention(4, dim, dim, dim, dropout=dropout)\n",
        "        self.text_image_pos_ffn = PositionwiseFeedForward(dim, dim*2, dropout=dropout)\n",
        "        self.image_text_attention = MultiHeadAttention(4, dim, dim, dim, dropout=dropout)\n",
        "        self.image_text_pos_ffn = PositionwiseFeedForward(dim, dim*2, dropout=dropout)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(dim*12+2, dim),\n",
        "            Mish(),\n",
        "            # nn.ReLU(),\n",
        "            nn.Linear(dim, 128),\n",
        "            Mish(),\n",
        "            # nn.ReLU(),\n",
        "            nn.Linear(128, 5)\n",
        "        )\n",
        "\n",
        "    def forward(self, claim_text, claim_image, document_text, document_image):\n",
        "\n",
        "        txt_sim = torch.tensor([util.pytorch_cos_sim(claim_text[i], document_text[i])[0][0] for i in range(claim_text.shape[0])]).reshape((claim_text.shape[0], 1)).cuda()\n",
        "        img_sim = torch.tensor([util.pytorch_cos_sim(claim_image[i], document_image[i])[0][0] for i in range(claim_text.shape[0])]).reshape((claim_text.shape[0], 1)).cuda()\n",
        "        # i_cluster = i_cluster.reshape((claim_text.shape[0], 1)).cuda()\n",
        "        \n",
        "        # transform to embeddings\n",
        "        claim_text_embedding = self.text_embedding(claim_text)\n",
        "        claim_image_embedding = self.image_embedding(claim_image)\n",
        "        document_text_embedding = self.document_text_embedding(document_text)\n",
        "        document_image_embedding = self.document_image_embedding(document_image)\n",
        "\n",
        "        # claim-document attention\n",
        "        claim_document_text, _ = self.claim_document_text_attention(claim_text_embedding, document_text_embedding, document_text_embedding)\n",
        "        claim_document_text = self.claim_document_text_pos_ffn(claim_document_text)\n",
        "        document_claim_text, _ = self.claim_document_text_attention(document_text_embedding, claim_text_embedding, claim_text_embedding)\n",
        "        document_claim_text = self.claim_document_text_pos_ffn(document_claim_text)\n",
        "\n",
        "        claim_document_image, _ = self.claim_document_image_attention(claim_image_embedding, document_image_embedding, document_image_embedding)\n",
        "        claim_document_image = self.claim_document_image_pos_ffn(claim_document_image)\n",
        "        document_claim_image, _ = self.claim_document_image_attention(document_image_embedding, claim_image_embedding, claim_image_embedding)\n",
        "        document_claim_image = self.claim_document_image_pos_ffn(document_claim_image)\n",
        "\n",
        "        # text-image co-attention\n",
        "        claim_text_image, _ = self.text_image_attention(claim_text_embedding, claim_image_embedding, claim_image_embedding)\n",
        "        claim_text_image = self.text_image_pos_ffn(claim_text_image)\n",
        "        claim_image_text, _ = self.image_text_attention(claim_image_embedding, claim_text_embedding, claim_text_embedding)\n",
        "        claim_image_text = self.image_text_pos_ffn(claim_image_text)\n",
        "\n",
        "        document_text_image, _ = self.text_image_attention(document_text_embedding, document_image_embedding, document_image_embedding)\n",
        "        document_text_image = self.text_image_pos_ffn(document_text_image)\n",
        "        document_image_text, _ = self.image_text_attention(document_image_embedding, document_text_embedding, document_text_embedding)\n",
        "        document_image_text = self.image_text_pos_ffn(document_image_text)\n",
        "\n",
        "        # aggregate word and image embedding to sentence embedding\n",
        "        claim_document_text = torch.mean(claim_document_text, dim=1)\n",
        "        document_claim_text = torch.mean(document_claim_text, dim=1)\n",
        "        claim_document_image = torch.mean(claim_document_image, dim=1)\n",
        "        document_claim_image = torch.mean(document_claim_image, dim=1)\n",
        "        claim_text_embedding = torch.mean(claim_text_embedding, dim=1)\n",
        "        document_text_embedding = torch.mean(document_text_embedding, dim=1)\n",
        "        claim_image_embedding = torch.mean(claim_image_embedding, dim=1)\n",
        "        document_image_embedding = torch.mean(document_image_embedding, dim=1)\n",
        "\n",
        "        claim_text_image = torch.mean(claim_text_image, dim=1)\n",
        "        claim_image_text = torch.mean(claim_image_text, dim=1)\n",
        "        document_text_image = torch.mean(document_text_image, dim=1)\n",
        "        document_image_text = torch.mean(document_image_text, dim=1)\n",
        "\n",
        "        \n",
        "        concat_embeddings = torch.cat((claim_text_embedding, claim_image_embedding,\n",
        "                                       document_text_embedding, document_image_embedding,\n",
        "                                       claim_document_text, document_claim_text,\n",
        "                                       claim_document_image, document_claim_image,\n",
        "                                       claim_text_image, claim_image_text,\n",
        "                                       document_text_image, document_image_text), dim=-1)\n",
        "        \n",
        "        # print(concat_embeddings.shape, txt_sim.shape, img_sim.shape)\n",
        "        # print(i_cluster.shape)\n",
        "\n",
        "        concat_embeddings = torch.cat((concat_embeddings, txt_sim, img_sim),dim=1)\n",
        "\n",
        "        predicted_output = self.classifier(concat_embeddings)\n",
        "        return predicted_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "jgvUi9E1eZLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add imports"
      ],
      "metadata": {
        "id": "9bLyIoI9vX58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTModel\n",
        "from transformers import DebertaTokenizer, DebertaModel\n",
        "# from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "import pandas as pd\n",
        "import logging\n",
        "import argparse\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.metrics import f1_score\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "8ixqxfAzGn8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Dataloader"
      ],
      "metadata": {
        "id": "Ij9ZyZMIvaRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, mode='train', dir='/content/drive/MyDrive/FactifyData/', read=10, transform=None):\n",
        "        self.mode = mode\n",
        "        self.dir = dir\n",
        "        self.transform = transform\n",
        "        if self.transform == None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "\n",
        "        category = {\n",
        "            'Support_Multimodal': 0,\n",
        "            'Support_Text': 1,\n",
        "            'Insufficient_Multimodal': 2,\n",
        "            'Insufficient_Text': 3,\n",
        "            'Refute': 4\n",
        "        }\n",
        "\n",
        "        if read<100:\n",
        "            self.content = pd.read_csv(dir + 'Clean_Data/' + mode + '_clean_' + str(read) + '.csv')\n",
        "        else:\n",
        "            self.content = pd.read_csv(dir + 'Clean_Data/' + mode + '_clean.csv')\n",
        "            self.content['Label'] = self.content['Category'].map(category)\n",
        "            self.content = self.content.dropna(subset=['claim', 'document'])\n",
        "\n",
        "        self.content.reset_index(inplace=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.content.Label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.dir + 'Factify_Data/Factify_Images/' + self.mode + '/'\n",
        "        if self.mode == 'train':\n",
        "            filename = path + 'claim/' + self.content['Category'][idx] + '/' + str(self.content.Id[idx]) + '.jpg'\n",
        "            input_claim_image = Image.open(filename)\n",
        "            claim_image = self.transform(input_claim_image)\n",
        "            claim = self.content.claim[idx]\n",
        "\n",
        "            filename = path + 'document/' + self.content['Category'][idx] + '/' + str(self.content.Id[idx]) + '.jpg'\n",
        "            input_document_image = Image.open(filename)\n",
        "            document_image = self.transform(input_document_image)\n",
        "            document = self.content.document[idx]\n",
        "\n",
        "            label = self.content.Label[idx]\n",
        "\n",
        "        elif self.mode == 'val':\n",
        "            filename = path + 'claim/' + str(self.content.Id[idx]) + '.jpg'\n",
        "            input_claim_image = Image.open(filename)\n",
        "            claim_image = self.transform(input_claim_image)\n",
        "            claim = self.content.claim[idx]\n",
        "\n",
        "            filename = path + 'document/' + str(self.content.Id[idx]) + '.jpg'\n",
        "            input_document_image = Image.open(filename)\n",
        "            document_image = self.transform(input_document_image)\n",
        "            document = self.content.document[idx]\n",
        "\n",
        "            label = self.content.Label[idx]\n",
        "\n",
        "        return claim, claim_image.numpy(), document, document_image.numpy(), label"
      ],
      "metadata": {
        "id": "aRu79QJ-GlA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import embedding models"
      ],
      "metadata": {
        "id": "qf42TWiDvdD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.ERROR)\n",
        "\n",
        "MODEL_TYPE = \"deberta\"\n",
        "PRETRAINED_PATH = 'microsoft/deberta-base'\n",
        "# PRETRAINED_PATH = 'sentence-transformers/all-mpnet-base-v2'\n",
        "# PRETRAINED_PATH = 'xlm-roberta-base'\n",
        "CV_PRETRAINED_PATH = 'facebook/deit-base-patch16-224'\n",
        "# CV_PRETRAINED_PATH = 'facebook/dino-vitb8'\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "h_zfUosuGknl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "HCIc-empvkn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed_value):\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)    # gpu vars"
      ],
      "metadata": {
        "id": "dkt3ZVtevnBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save(model, epoch=None):\n",
        "    output_folder_name = '/content/drive/MyDrive/FactifyData/models/'\n",
        "    if not os.path.exists(output_folder_name):\n",
        "        os.makedirs(output_folder_name)\n",
        "\n",
        "    if epoch is None:\n",
        "        model_name = output_folder_name + 'model'\n",
        "        config_name = output_folder_name + 'config'\n",
        "    else:\n",
        "        model_name = output_folder_name + str(epoch) + 'model'\n",
        "        config_name = output_folder_name + str(epoch) + 'config'\n",
        "    \n",
        "    torch.save(model.state_dict(), model_name)"
      ],
      "metadata": {
        "id": "5uXYgspjGuGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load pretrained models, Criterion and Optim"
      ],
      "metadata": {
        "id": "vMjb-uNIvtHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "# load pretrained NLP model\n",
        "deberta_tokenizer = DebertaTokenizer.from_pretrained(PRETRAINED_PATH)\n",
        "deberta = DebertaModel.from_pretrained(PRETRAINED_PATH)\n",
        "for param in deberta.parameters():\n",
        "    param.requires_grad = False\n",
        "#mpnet = SentenceTransformer(PRETRAINED_PATH)\n",
        "\n",
        "vit_model = ViTModel.from_pretrained(CV_PRETRAINED_PATH)\n",
        "for param in vit_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "fake_net = FakeNet()\n",
        "\n",
        "# fake_net.load_state_dict(torch.load('model/20211101-002023_/19model'))\n",
        "\n",
        "lr = 2e-5\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()   \n",
        "fake_net_optimizer = torch.optim.Adam(fake_net.parameters(), lr=lr)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(fake_net_optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "deberta.to(device)\n",
        "# mpnet.to(device)\n",
        "vit_model.to(device)\n",
        "fake_net.to(device)\n",
        "criterion.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2V99-zqUuL_",
        "outputId": "3c01f035-86d1-42fb-e966-5e2ae1349889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Batch"
      ],
      "metadata": {
        "id": "prV3Btobv3qK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 24\n",
        "subset = 10\n",
        "\n",
        "train_dataset = MultiModalDataset(mode='train', read=subset)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataset = MultiModalDataset(mode='val', read=subset)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# print(sum(p.numel() for p in deberta.parameters() if p.requires_grad))\n",
        "# print(sum(p.numel() for p in vgg19_model.parameters() if p.requires_grad))\n",
        "print(sum(p.numel() for p in fake_net.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrfTuFEqv2l9",
        "outputId": "dc2618e6-e573-4f3d-c00f-468ef39d8226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25774341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deit_deberta_10_train_loss = []\n",
        "# deit_deberta_10_train_f1 = []\n",
        "# deit_deberta_10_train_acc = []\n",
        "# deit_deberta_10_train_pre = []\n",
        "# deit_deberta_10_train_rec = []\n",
        "\n",
        "deit_deberta_10_val_loss = []\n",
        "deit_deberta_10_val_f1 = []\n",
        "deit_deberta_10_val_acc = []\n",
        "deit_deberta_10_val_pre = []\n",
        "deit_deberta_10_val_rec = []"
      ],
      "metadata": {
        "id": "vG6RZph1aZTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "IOa2RsOTwN6O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX72KbypfhZt",
        "outputId": "6039031e-14fb-48fb-8f89-848e2dfbbc64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.476: 100%|██████████| 5/5 [51:26<00:00, 617.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60.550128161907196 0.59813\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "\n",
        "# training\n",
        "best_val_f1 = 0\n",
        "pbar = tqdm(range(epochs), desc='Epoch: ')\n",
        "for epoch in pbar:\n",
        "    fake_net.train()\n",
        "    total_loss = 0\n",
        "    for loader_idx, item in enumerate(train_dataloader):\n",
        "        fake_net_optimizer.zero_grad()\n",
        "        claim_text, claim_image, document_text, document_image, label = item[0], item[1].to(device), item[2], item[3].to(device), item[4].to(device)\n",
        "\n",
        "        # print(loader_idx, claim_text, document_text, label)\n",
        "        # transform sentences to embeddings via DeBERTa\n",
        "        input_claim = deberta_tokenizer(claim_text, truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_SEQUENCE_LENGTH).to(device)\n",
        "        output_claim = deberta(**input_claim)\n",
        "        output_claim_text = output_claim.last_hidden_state\n",
        "        # output_claim_text = torch.from_numpy(mpnet.encode(claim_text)).cuda()\n",
        "        # print(output_claim_text.shape)\n",
        "\n",
        "        input_document = deberta_tokenizer(document_text, truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_SEQUENCE_LENGTH).to(device)\n",
        "        output_document = deberta(**input_document)\n",
        "        output_document_text = output_document.last_hidden_state\n",
        "        # output_document_text = torch.from_numpy(mpnet.encode(document_text)).cuda()\n",
        "\n",
        "        # input_claim_image = feature_extractor(images=claim_image, return_tensors=\"pt\").to(device)\n",
        "        output_claim_image = vit_model(claim_image)\n",
        "        output_claim_image = output_claim_image.last_hidden_state\n",
        "\n",
        "        # input_document_image = feature_extractor(images=document_image, return_tensors=\"pt\").to(device)\n",
        "        output_document_image = vit_model(document_image)\n",
        "        output_document_image = output_document_image.last_hidden_state\n",
        "\n",
        "        predicted_output = fake_net(output_claim_text, output_claim_image, output_document_text, output_document_image)\n",
        "        \n",
        "        loss = criterion(predicted_output, label)\n",
        "        loss.backward()\n",
        "        fake_net_optimizer.step()\n",
        "\n",
        "        current_loss = loss.item()\n",
        "        total_loss += current_loss\n",
        "        pbar.set_description(\"Loss: {}\".format(round(current_loss, 3)), refresh=True)\n",
        "    # scheduler.step()\n",
        "    deit_deberta_10_train_loss.append(total_loss/len(train_dataloader))\n",
        "\n",
        "    # testing\n",
        "    y_pred, y_true = [], []\n",
        "    total_val_loss = 0\n",
        "    fake_net.eval()\n",
        "    for loader_idx, item in enumerate(val_dataloader):\n",
        "        claim_text, claim_image, document_text, document_image, label = item[0], item[1].to(device), item[2], item[3].to(device), item[4].to(device)\n",
        "\n",
        "        # transform sentences to embeddings via DeBERTa\n",
        "        input_claim = deberta_tokenizer(claim_text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
        "        output_claim = deberta(**input_claim)\n",
        "        output_claim_text = output_claim.last_hidden_state\n",
        "        # output_claim_text = torch.from_numpy(mpnet.encode(claim_text)).cuda()\n",
        "\n",
        "        input_document = deberta_tokenizer(document_text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
        "        output_document = deberta(**input_document)\n",
        "        output_document_text = output_document.last_hidden_state\n",
        "        # output_document_text = torch.from_numpy(mpnet.encode(document_text)).cuda()\n",
        "\n",
        "        output_claim_image = vit_model(claim_image)\n",
        "        output_claim_image = output_claim_image.last_hidden_state\n",
        "\n",
        "        output_document_image = vit_model(document_image)\n",
        "        output_document_image = output_document_image.last_hidden_state\n",
        "\n",
        "        predicted_output = fake_net(output_claim_text, output_claim_image, output_document_text, output_document_image)\n",
        "        \n",
        "        _, predicted_label = torch.topk(predicted_output, 1)\n",
        "\n",
        "        loss = criterion(predicted_output, label)\n",
        "\n",
        "        current_val_loss = loss.item()\n",
        "        total_val_loss += current_val_loss\n",
        "\n",
        "        if len(y_pred) == 0:\n",
        "            y_pred = predicted_label.cpu().detach().flatten().tolist()\n",
        "            y_true = label.tolist()\n",
        "        else:\n",
        "            y_pred += predicted_label.cpu().detach().flatten().tolist()\n",
        "            y_true += label.tolist()\n",
        "\n",
        "    f1 = round(f1_score(y_true, y_pred, average='weighted'), 5)\n",
        "    acc = round(accuracy_score(y_true, y_pred), 5)\n",
        "    pre = round(precision_score(y_true, y_pred, average='weighted'), 5)\n",
        "    rec = round(recall_score(y_true, y_pred, average='weighted'), 5)\n",
        "\n",
        "    deit_deberta_10_val_loss.append(total_val_loss/len(val_dataloader))\n",
        "    deit_deberta_10_val_f1.append(f1)\n",
        "    deit_deberta_10_val_acc.append(acc)\n",
        "    deit_deberta_10_val_pre.append(pre)\n",
        "    deit_deberta_10_val_rec.append(rec)\n",
        "\n",
        "    if f1 >= best_val_f1:\n",
        "        best_val_f1 = f1\n",
        "        save(fake_net, epoch=epoch)\n",
        "\n",
        "    with open('/content/drive/MyDrive/FactifyData/models/' + 'record', 'a') as config_file:\n",
        "        config_file.write(str(epoch) + ',' + str(round(total_loss/len(train_dataloader), 5)) + ',' + str(f1))\n",
        "        config_file.write('\\n')\n",
        "\n",
        "print(total_loss, best_val_f1)\n",
        "save(fake_net) "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sCQTbSdg5_BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MRMRqlfT_tJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uRUMUWIj_tGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5qvn6Umy_tDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(deit_deberta_10_train_loss)\n",
        "\n",
        "print(deit_deberta_10_val_loss)\n",
        "print(deit_deberta_10_val_f1)\n",
        "print(deit_deberta_10_val_acc)\n",
        "print(deit_deberta_10_val_pre)\n",
        "print(deit_deberta_10_val_rec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckOwVodV9Ioj",
        "outputId": "f72a2d64-893c-41bc-d169-7f6629b46085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.2712399461497998, 1.0393108807197988, 0.9281675162380689, 0.8419507047901414, 0.7562382041591488, 0.6860600873215558, 0.6134326833568208, 0.5388264841821095, 0.4759993972639515, 0.41472690521854244]\n",
            "[1.1230518240481615, 1.0141943898051977, 1.0020177513360977, 0.942060548812151, 0.9371830271556973, 0.9637178406119347, 0.9766250690445304, 1.0200954591855407, 1.1395680056884885, 1.1777006518095732]\n",
            "[0.48462, 0.55576, 0.52123, 0.58408, 0.58563, 0.59432, 0.59813, 0.5942, 0.58478, 0.58555]\n",
            "[0.512, 0.56533, 0.548, 0.58667, 0.588, 0.59467, 0.59867, 0.596, 0.596, 0.59067]\n",
            "[0.53647, 0.55574, 0.59025, 0.59653, 0.60119, 0.62506, 0.60126, 0.6002, 0.60118, 0.58689]\n",
            "[0.512, 0.56533, 0.548, 0.58667, 0.588, 0.59467, 0.59867, 0.596, 0.596, 0.59067]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision_recall_fscore_support(y_true, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCVz9UyD9IVn",
        "outputId": "1839acf3-1867-4dbd-9322-4450b9a104c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.56321839, 0.46902655, 0.44886364, 0.49253731, 0.96078431]),\n",
              " array([0.65333333, 0.35333333, 0.52666667, 0.44      , 0.98      ]),\n",
              " array([0.60493827, 0.40304183, 0.48466258, 0.46478873, 0.97029703]),\n",
              " array([150, 150, 150, 150, 150]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "  \n",
        "  \n",
        "  \n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot([str(i) for i in range(1, 11)], deit_deberta_10_train_loss, color = 'orange',\n",
        "         linestyle = 'solid')\n",
        "plt.plot([str(i) for i in range(1, 11)], deit_deberta_10_val_loss, color = 'blue',\n",
        "         linestyle = 'solid')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "LMie_ukb_j5e",
        "outputId": "c473ba01-fc27-4918-fc2a-87132b54de2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fec1a903410>]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iQHflbr8_sWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision 0.56321839 0.46902655 0.44886364 0.49253731 0.96078431\n",
        "recall    0.65333333 0.35333333 0.52666667 0.44       0.98      \n",
        "fscore    0.60493827 0.40304183 0.48466258 0.46478873 0.97029703"
      ],
      "metadata": {
        "id": "6CIAcb72_sTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "spx-VS9i_sPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DO NOT RUN"
      ],
      "metadata": {
        "id": "l9nFqJwywXIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(deit_deberta_10_train_loss)\n",
        "\n",
        "print(deit_deberta_10_val_loss)\n",
        "print(deit_deberta_10_val_f1)\n",
        "print(deit_deberta_10_val_acc)\n",
        "print(deit_deberta_10_val_pre)\n",
        "print(deit_deberta_10_val_rec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqMkPL-ffkvV",
        "outputId": "df95b4b2-984c-4c8b-dc5a-1ce3a2c9f0d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.447403289590563, 1.1924490077154977, 1.0781186010156358, 0.9883676256452288, 0.9137181511947087, 0.8355393673692431, 0.7625563617263521, 0.6625599712133408, 0.5860830055815833, 0.4964818543621472]\n",
            "[1.316380262374878, 1.203302256266276, 1.1254127383232118, 1.0945040225982665, 1.080694603919983, 1.0468825737635294, 1.0396431724230448, 1.056967846552531, 1.0699144721031189, 1.1057071407636008]\n",
            "[0.43637, 0.47018, 0.47731, 0.50625, 0.46864, 0.52428, 0.53325, 0.55333, 0.56512, 0.54147]\n",
            "[0.456, 0.496, 0.488, 0.52267, 0.488, 0.528, 0.53067, 0.55467, 0.56533, 0.544]\n",
            "[0.45455, 0.50321, 0.48419, 0.53095, 0.48489, 0.54931, 0.56515, 0.57837, 0.57989, 0.55344]\n",
            "[0.456, 0.496, 0.488, 0.52267, 0.488, 0.528, 0.53067, 0.55467, 0.56533, 0.544]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision_recall_fscore_support(y_true, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxhU8cXmuax3",
        "outputId": "cc04a661-49cb-429e-8c14-f831bf4a905f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.60294118, 0.44897959, 0.38028169, 0.375     , 0.96      ]),\n",
              " array([0.54666667, 0.29333333, 0.36      , 0.56      , 0.96      ]),\n",
              " array([0.57342657, 0.35483871, 0.36986301, 0.44919786, 0.96      ]),\n",
              " array([75, 75, 75, 75, 75]))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# facebook/roscoe-512-roberta-base\n",
        "# facebook/vit-msn-base\n",
        "# facebook/regnet-y-320-seer\n",
        "# facebook/dino-vitb16\n",
        "# facebook/bart-large-xsum"
      ],
      "metadata": {
        "id": "khI45qK7gbHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/MyDrive/FactifyData/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHm9DUOLTJ7E",
        "outputId": "da88f73d-10f7-4b7e-9d92-1ab0e102c019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean_Data  Factify_Data  models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VfGEoERjTQzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset.to_csv('/content/drive/MyDrive/FactifyData/Clean_Data/train_clean_exp_'+str(round(read*100))+'.csv', index=False)"
      ],
      "metadata": {
        "id": "h3xgbsjfxvfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment with embeddings"
      ],
      "metadata": {
        "id": "T7TA5Vvjej37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "5BQ9fs3-eqfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "QKwhufgHfifn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "read = 0.01        # 1% data"
      ],
      "metadata": {
        "id": "huC0IyJCfOOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = pd.read_csv('/content/drive/MyDrive/FactifyData/Clean_Data/train_clean_'+str(round(read*100))+'.csv')\n",
        "valset = pd.read_csv('/content/drive/MyDrive/FactifyData/Clean_Data/val_clean_'+str(round(read*100))+'.csv')"
      ],
      "metadata": {
        "id": "90k59qHvfLOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset"
      ],
      "metadata": {
        "id": "Eu_-iRaIfmSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "def add_sent_embs(data):\n",
        "    data['claim_emb'] = ''\n",
        "    data['doc_emb'] = ''\n",
        "    data['sim_text'] = ''\n",
        "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        sentences = [data.claim[i], data.document[i]]\n",
        "        embeddings = model.encode(sentences)\n",
        "        sim = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
        "\n",
        "        data['claim_emb'].iloc[i], data['doc_emb'].iloc[i] = embeddings[0], embeddings[1]\n",
        "        data['sim_text'].iloc[i] = sim[0][0]\n",
        "        \n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "6u7Otho5ewYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTFeatureExtractor, ViTModel\n",
        "import torch\n",
        "\n",
        "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "inputs = feature_extractor(image, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "last_hidden_states.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF8VQi4Zi1CI",
        "outputId": "7f0721ea-4e94-43bc-c01b-9fb32a3ca312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clipmodel = 'clip-ViT-B-32'\n",
        "\n",
        "model = SentenceTransformer(clipmodel)\n",
        "print(\"We will use model:\", clipmodel)"
      ],
      "metadata": {
        "id": "75_pG-B8m8rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_sim(data, mode='val'):\n",
        "    if mode=='val':\n",
        "        claimlist = ['/content/drive/MyDrive/FactifyData/Factify_Data/Factify_Images/val/claim/' + str(data.Id[idx]) + '.jpg' for idx in range(len(data))]\n",
        "        doclist = ['/content/drive/MyDrive/FactifyData/Factify_Data/Factify_Images/val/document/' + str(data.Id[idx]) + '.jpg' for idx in range(len(data))]\n",
        "    elif mode == 'train':\n",
        "        claimlist = ['/content/drive/MyDrive/FactifyData/Factify_Data/Factify_Images/train/claim/' + data['Category'][idx] + '/' + str(data.Id[idx]) + '.jpg' for idx in range(len(data))]\n",
        "        doclist = ['/content/drive/MyDrive/FactifyData/Factify_Data/Factify_Images/train/document/'+ data['Category'][idx] + '/' + str(data.Id[idx]) + '.jpg' for idx in range(len(data))]\n",
        "    \n",
        "    claim_emb = model.encode([Image.open(filename) \n",
        "                            for filename in claimlist],\n",
        "                            batch_size=64,\n",
        "                            convert_to_tensor=True,\n",
        "                            show_progress_bar=True)\n",
        "    \n",
        "    doc_emb = model.encode([Image.open(filename) \n",
        "                            for filename in doclist],\n",
        "                            batch_size=64,\n",
        "                            convert_to_tensor=True,\n",
        "                           show_progress_bar=True)\n",
        "\n",
        "    sim = [util.pytorch_cos_sim(claim_emb[i], doc_emb[i])[0][0].cpu() for i in range(len(data))]\n",
        "    data['img_claim_emb'] = ''\n",
        "    data['img_doc_emb'] = ''\n",
        "    for i in range(len(data)):\n",
        "        data['img_claim_emb'].iloc[i], data['img_doc_emb'].iloc[i] = claim_emb[i].cpu(), doc_emb[i].cpu()\n",
        "    data['sim_img'] = sim\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "441d9sa-nBZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valset = add_sent_embs(valset)"
      ],
      "metadata": {
        "id": "0Se4Yvs-ezEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valset = image_sim(valset)"
      ],
      "metadata": {
        "id": "c8NyLskGeyiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valset.to_csv('/content/drive/MyDrive/FactifyData/Clean_Data/val_clean_exp_'+str(round(read*100))+'.csv', index=False)"
      ],
      "metadata": {
        "id": "s1hjjmNFxQs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = add_sent_embs(trainset)\n",
        "trainset = image_sim(trainset, mode='train')"
      ],
      "metadata": {
        "id": "Rh_oF9_PxZZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainnnn"
      ],
      "metadata": {
        "id": "YJ3Cfghr1Jz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTModel\n",
        "from transformers import DebertaTokenizer, DebertaModel\n",
        "# from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "import pandas as pd\n",
        "import logging\n",
        "import argparse\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.metrics import f1_score\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "xsJ05LL81LND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    ''' Scaled Dot-Product Attention '''\n",
        "\n",
        "    def __init__(self, temperature, attn_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
        "\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
        "        output = torch.matmul(attn, v)\n",
        "\n",
        "        return output, attn\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    ''' Multi-Head Attention module '''\n",
        "\n",
        "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "\n",
        "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
        "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
        "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
        "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
        "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
        "\n",
        "        residual = q\n",
        "\n",
        "        # Pass through the pre-attention projection: b x lq x (n*dv)\n",
        "        # Separate different heads: b x lq x n x dv\n",
        "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
        "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
        "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
        "\n",
        "        # Transpose for attention dot product: b x n x lq x dv\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
        "\n",
        "        q, attn = self.attention(q, k, v, mask=mask)\n",
        "\n",
        "        # Transpose to move the head dimension back: b x lq x n x dv\n",
        "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
        "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
        "        q = self.dropout(self.fc(q))\n",
        "        q += residual\n",
        "\n",
        "        q = self.layer_norm(q)\n",
        "\n",
        "        return q, attn\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    ''' A two-feed-forward-layer module '''\n",
        "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.w_1 = nn.Linear(d_in, d_hid)\n",
        "        self.w_2 = nn.Linear(d_hid, d_in)\n",
        "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        x = self.w_2(F.gelu(self.w_1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x += residual\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "SMvoDLjB2RkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Mish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.tanh(F.softplus(x))\n",
        "\n",
        "\n",
        "class FakeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        dim = 512\n",
        "        dropout = 0.1\n",
        "        head = 4\n",
        "\n",
        "        self.text_embedding = nn.Sequential(\n",
        "            nn.Linear(768, dim),\n",
        "            # Mish()\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.document_text_embedding = nn.Sequential(\n",
        "            nn.Linear(768, dim),\n",
        "            # Mish()\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.image_embedding = nn.Sequential(\n",
        "            nn.Linear(768, dim),\n",
        "            # Mish()\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.document_image_embedding = nn.Sequential(\n",
        "            nn.Linear(768, dim),\n",
        "            # Mish()\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.claim_document_text_attention = MultiHeadAttention(head, dim, dim, dim, dropout=dropout)\n",
        "        self.claim_document_text_pos_ffn = PositionwiseFeedForward(dim, dim*2, dropout=dropout)\n",
        "\n",
        "        self.claim_document_image_attention = MultiHeadAttention(head, dim, dim, dim, dropout=dropout)\n",
        "        self.claim_document_image_pos_ffn = PositionwiseFeedForward(dim, dim*2, dropout=dropout)\n",
        "\n",
        "        self.text_image_attention = MultiHeadAttention(4, dim, dim, dim, dropout=dropout)\n",
        "        self.text_image_pos_ffn = PositionwiseFeedForward(dim, dim*2, dropout=dropout)\n",
        "        self.image_text_attention = MultiHeadAttention(4, dim, dim, dim, dropout=dropout)\n",
        "        self.image_text_pos_ffn = PositionwiseFeedForward(dim, dim*2, dropout=dropout)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(dim*12+2, dim),\n",
        "            # Mish(),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim, 128),\n",
        "            # Mish(),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 5)\n",
        "        )\n",
        "\n",
        "    def forward(self, claim_text, claim_image, document_text, document_image, img_sim, txt_sim):\n",
        "        # transform to embeddings\n",
        "        claim_text_embedding = self.text_embedding(claim_text)\n",
        "        claim_image_embedding = self.image_embedding(claim_image)\n",
        "        document_text_embedding = self.document_text_embedding(document_text)\n",
        "        document_image_embedding = self.document_image_embedding(document_image)\n",
        "\n",
        "        # claim-document attention\n",
        "        claim_document_text, _ = self.claim_document_text_attention(claim_text_embedding, document_text_embedding, document_text_embedding)\n",
        "        claim_document_text = self.claim_document_text_pos_ffn(claim_document_text)\n",
        "        document_claim_text, _ = self.claim_document_text_attention(document_text_embedding, claim_text_embedding, claim_text_embedding)\n",
        "        document_claim_text = self.claim_document_text_pos_ffn(document_claim_text)\n",
        "\n",
        "        claim_document_image, _ = self.claim_document_image_attention(claim_image_embedding, document_image_embedding, document_image_embedding)\n",
        "        claim_document_image = self.claim_document_image_pos_ffn(claim_document_image)\n",
        "        document_claim_image, _ = self.claim_document_image_attention(document_image_embedding, claim_image_embedding, claim_image_embedding)\n",
        "        document_claim_image = self.claim_document_image_pos_ffn(document_claim_image)\n",
        "\n",
        "        # text-image co-attention\n",
        "        claim_text_image, _ = self.text_image_attention(claim_text_embedding, claim_image_embedding, claim_image_embedding)\n",
        "        claim_text_image = self.text_image_pos_ffn(claim_text_image)\n",
        "        claim_image_text, _ = self.image_text_attention(claim_image_embedding, claim_text_embedding, claim_text_embedding)\n",
        "        claim_image_text = self.image_text_pos_ffn(claim_image_text)\n",
        "\n",
        "        document_text_image, _ = self.text_image_attention(document_text_embedding, document_image_embedding, document_image_embedding)\n",
        "        document_text_image = self.text_image_pos_ffn(document_text_image)\n",
        "        document_image_text, _ = self.image_text_attention(document_image_embedding, document_text_embedding, document_text_embedding)\n",
        "        document_image_text = self.image_text_pos_ffn(document_image_text)\n",
        "\n",
        "        # aggregate word and image embedding to sentence embedding\n",
        "        claim_document_text = torch.mean(claim_document_text, dim=1)\n",
        "        document_claim_text = torch.mean(document_claim_text, dim=1)\n",
        "        claim_document_image = torch.mean(claim_document_image, dim=1)\n",
        "        document_claim_image = torch.mean(document_claim_image, dim=1)\n",
        "        claim_text_embedding = torch.mean(claim_text_embedding, dim=1)\n",
        "        document_text_embedding = torch.mean(document_text_embedding, dim=1)\n",
        "        claim_image_embedding = torch.mean(claim_image_embedding, dim=1)\n",
        "        document_image_embedding = torch.mean(document_image_embedding, dim=1)\n",
        "\n",
        "        claim_text_image = torch.mean(claim_text_image, dim=1)\n",
        "        claim_image_text = torch.mean(claim_image_text, dim=1)\n",
        "        document_text_image = torch.mean(document_text_image, dim=1)\n",
        "        document_image_text = torch.mean(document_image_text, dim=1)\n",
        "\n",
        "        \n",
        "        concat_embeddings = torch.cat((claim_text_embedding, claim_image_embedding,\n",
        "                                       document_text_embedding, document_image_embedding,\n",
        "                                       claim_document_text, document_claim_text,\n",
        "                                       claim_document_image, document_claim_image,\n",
        "                                       claim_text_image, claim_image_text,\n",
        "                                       document_text_image, document_image_text, img_sim, txt_sim), dim=-1)\n",
        "\n",
        "        predicted_output = self.classifier(concat_embeddings)\n",
        "        return predicted_output"
      ],
      "metadata": {
        "id": "-axuN1j_2U18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, mode='train', dir='/content/drive/MyDrive/FactifyData/', read=10, transform=None):\n",
        "        self.mode = mode\n",
        "        self.dir = dir\n",
        "        self.transform = transform\n",
        "        if self.transform == None:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "\n",
        "        category = {\n",
        "            'Support_Multimodal': 0,\n",
        "            'Support_Text': 1,\n",
        "            'Insufficient_Multimodal': 2,\n",
        "            'Insufficient_Text': 3,\n",
        "            'Refute': 4\n",
        "        }\n",
        "\n",
        "        if read<100:\n",
        "            self.content = pd.read_csv(dir + 'Clean_Data/' + mode + '_clean_' + str(read) + '.csv')\n",
        "        else:\n",
        "            self.content = pd.read_csv(dir + 'Clean_Data/' + mode + '_clean.csv')\n",
        "            self.content['Label'] = self.content['Category'].map(category)\n",
        "            self.content = self.content.dropna(subset=['claim', 'document'])\n",
        "\n",
        "        self.content.reset_index(inplace=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.content.Label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.dir + 'Factify_Data/Factify_Images/' + self.mode + '/'\n",
        "        if self.mode == 'train':\n",
        "            filename = path + 'claim/' + self.content['Category'][idx] + '/' + str(self.content.Id[idx]) + '.jpg'\n",
        "            input_claim_image = Image.open(filename)\n",
        "            claim_image = self.transform(input_claim_image)\n",
        "            claim = self.content.claim[idx]\n",
        "\n",
        "            filename = path + 'document/' + self.content['Category'][idx] + '/' + str(self.content.Id[idx]) + '.jpg'\n",
        "            input_document_image = Image.open(filename)\n",
        "            document_image = self.transform(input_document_image)\n",
        "            document = self.content.document[idx]\n",
        "\n",
        "            label = self.content.Label[idx]\n",
        "\n",
        "        elif self.mode == 'val':\n",
        "            filename = path + 'claim/' + str(self.content.Id[idx]) + '.jpg'\n",
        "            input_claim_image = Image.open(filename)\n",
        "            claim_image = self.transform(input_claim_image)\n",
        "            claim = self.content.claim[idx]\n",
        "\n",
        "            filename = path + 'document/' + str(self.content.Id[idx]) + '.jpg'\n",
        "            input_document_image = Image.open(filename)\n",
        "            document_image = self.transform(input_document_image)\n",
        "            document = self.content.document[idx]\n",
        "\n",
        "            label = self.content.Label[idx]\n",
        "            \n",
        "        img_sim, txt_sim = self.content.sim_img[idx], self.content.sim_text[idx]\n",
        "\n",
        "\n",
        "        return claim, claim_image.numpy(), document, document_image.numpy(), img_sim, txt_sim, label"
      ],
      "metadata": {
        "id": "_HtTH-to1Lyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.ERROR)\n",
        "\n",
        "MODEL_TYPE = \"deberta\"\n",
        "PRETRAINED_PATH = 'microsoft/deberta-base'\n",
        "# PRETRAINED_PATH = 'xlm-roberta-base'\n",
        "CV_PRETRAINED_PATH = 'facebook/deit-base-patch16-224'\n",
        "# CV_PRETRAINED_PATH = 'facebook/dino-vitb8'\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "_qf13K7S1tRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save(model, epoch=None):\n",
        "    output_folder_name = '/content/drive/MyDrive/FactifyData/models/'\n",
        "    if not os.path.exists(output_folder_name):\n",
        "        os.makedirs(output_folder_name)\n",
        "\n",
        "    if epoch is None:\n",
        "        model_name = output_folder_name + 'model'\n",
        "        config_name = output_folder_name + 'config'\n",
        "    else:\n",
        "        model_name = output_folder_name + str(epoch) + 'model'\n",
        "        config_name = output_folder_name + str(epoch) + 'config'\n",
        "    \n",
        "    torch.save(model.state_dict(), model_name)"
      ],
      "metadata": {
        "id": "HQxYMBIV2BFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed_value):\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)    # gpu vars"
      ],
      "metadata": {
        "id": "HGO67sWv2Eet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "# load pretrained NLP model\n",
        "deberta_tokenizer = DebertaTokenizer.from_pretrained(PRETRAINED_PATH)\n",
        "deberta = DebertaModel.from_pretrained(PRETRAINED_PATH)\n",
        "for param in deberta.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "vit_model = ViTModel.from_pretrained(CV_PRETRAINED_PATH)\n",
        "for param in vit_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "fake_net = FakeNet()\n",
        "\n",
        "# fake_net.load_state_dict(torch.load('model/20211101-002023_/19model'))\n",
        "\n",
        "lr = 2e-5\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()   \n",
        "fake_net_optimizer = torch.optim.Adam(fake_net.parameters(), lr=lr)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(fake_net_optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "deberta.to(device)\n",
        "vit_model.to(device)\n",
        "fake_net.to(device)\n",
        "criterion.to(device)"
      ],
      "metadata": {
        "id": "ep1N4Jar2G7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 25\n",
        "subset = 5\n",
        "\n",
        "train_dataset = MultiModalDataset(mode='train', read=subset)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataset = MultiModalDataset(mode='val', read=subset)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# print(sum(p.numel() for p in deberta.parameters() if p.requires_grad))\n",
        "# print(sum(p.numel() for p in vgg19_model.parameters() if p.requires_grad))\n",
        "print(sum(p.numel() for p in fake_net.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "id": "rg003Ju42Kse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "wYp1xIL_tNGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tset = pd.read_csv('/content/drive/MyDrive/FactifyData/Clean_Data/train_clean_1.csv')\n",
        "vset = pd.read_csv('/content/drive/MyDrive/FactifyData/Clean_Data/val_clean_1.csv')"
      ],
      "metadata": {
        "id": "J5x0cAy72Swa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tset['i_cluster'] = ''\n",
        "r = np.random.randint(0, 2, len(tset))\n",
        "r[:150] = 1\n",
        "\n",
        "tset.i_cluster = r * tset.Label"
      ],
      "metadata": {
        "id": "jTWw68L62pHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vset['i_cluster'] = ''\n",
        "r = np.random.randint(0, 2, len(vset))\n",
        "r[:50] = 1\n",
        "\n",
        "vset.i_cluster = r * vset.Label"
      ],
      "metadata": {
        "id": "Ke6BF0B83UfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tset.to_csv('/content/drive/MyDrive/FactifyData/Clean_Data/train_clean_1.csv')\n",
        "vset.to_csv('/content/drive/MyDrive/FactifyData/Clean_Data/val_clean_1.csv')"
      ],
      "metadata": {
        "id": "kV7y3OBr3esc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "aTrf_9C9cU6j",
        "SMQlKL9nnyZr"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}